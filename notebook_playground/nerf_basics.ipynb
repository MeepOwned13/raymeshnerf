{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow importing from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "COMPUTE_DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    COMPUTE_DEVICE = torch.device('cuda:0')\n",
    "elif torch.mps.is_available():\n",
    "    COMPUTE_DEVICE = torch.device('mps')\n",
    "print(f\"{COMPUTE_DEVICE=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.load(\"../data/tiny_nerf_data.npz\")\n",
    "images, c2ws, focal = data[\"images\"], data[\"poses\"], data[\"focal\"]\n",
    "\n",
    "print(\n",
    "    f\"Shapes:\",\n",
    "    f\"{images.shape=}\",\n",
    "    f\"{c2ws.shape=}\",\n",
    "    f\"{focal.shape=}\",\n",
    "    sep='\\n  ',\n",
    ")\n",
    "\n",
    "plt.imshow(images[2])\n",
    "plt.title(\"Image at index 2\")\n",
    "\n",
    "print(f\"C2W transform at index 2: \\n\", c2ws[2])\n",
    "\n",
    "print(f\"Focal length: {focal:.4f}\")\n",
    "\n",
    "images = torch.from_numpy(images)\n",
    "c2ws = torch.from_numpy(c2ws)\n",
    "focal = torch.from_numpy(focal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "To be exported into `ROOT_DIR/src/utils/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def create_rays(height: int, width: int, intrinsic: Tensor, c2w: Tensor):\n",
    "    device = c2w.device\n",
    "\n",
    "    focal_x = intrinsic[0, 0]\n",
    "    focal_y = intrinsic[1, 1]\n",
    "    # cx and cy handle the misalignement of the principal point with the center of the image\n",
    "    cx = intrinsic[0, 2]\n",
    "    cy = intrinsic[1, 2]\n",
    "\n",
    "    # Index each point on the image, determine ray directions to them\n",
    "    i, j = torch.meshgrid(torch.arange(width, dtype=torch.float32, device=device), torch.arange(height, dtype=torch.float32, device=device), indexing='xy')\n",
    "    directions = torch.stack((\n",
    "        (i - cx) / focal_x,\n",
    "        -(j - cy) / focal_y,\n",
    "        -torch.ones(i.shape, dtype=torch.float32, device=device)  # -1 since ray is cast away from camera\n",
    "    ), -1)\n",
    "\n",
    "    # Transform ray directions to World, origins just need to be broadcasted accordingly\n",
    "    ray_directions = F.normalize(directions @ c2w[:3, :3].T, \"fro\", -1)\n",
    "    ray_origins = torch.broadcast_to(c2w[:3, -1], ray_directions.shape)  # c2w last column determines position\n",
    "    \n",
    "    return ray_origins, ray_directions\n",
    "\n",
    "\n",
    "# Test on real data\n",
    "ex_index = 2\n",
    "ex_img = images[ex_index]\n",
    "ex_intr = torch.tensor([\n",
    "    [focal.item(), 0, ex_img.shape[1] // 2],\n",
    "    [0, focal.item(), ex_img.shape[0] // 2],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "origins, directions = create_rays(ex_img.shape[0], ex_img.shape[1], ex_intr.to(COMPUTE_DEVICE), c2ws[ex_index].to(COMPUTE_DEVICE))\n",
    "\n",
    "\"\"\"\n",
    "# Test on example data\n",
    "ex_intr = torch.tensor([\n",
    "    [4, 0, 5 // 2],\n",
    "    [0, 4, 5 // 2],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "ex_c2w = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "o, d = create_rays(5, 5, ex_intr, ex_c2w)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"{origins.shape=} | {directions.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: DATAMODULE\n",
    "def compute_near_far_planes(c2ws: Tensor):\n",
    "    scene_bounds_min = torch.tensor([-1, -1, -1], dtype=torch.float32)\n",
    "    scene_bounds_max = torch.tensor([1, 1, 1], dtype=torch.float32)\n",
    "\n",
    "    # Transform bounding box corners to camera coordinates\n",
    "    corners = torch.tensor([\n",
    "        [scene_bounds_min[0], scene_bounds_min[1], scene_bounds_min[2]],\n",
    "        [scene_bounds_min[0], scene_bounds_min[1], scene_bounds_max[2]],\n",
    "        [scene_bounds_min[0], scene_bounds_max[1], scene_bounds_min[2]],\n",
    "        [scene_bounds_min[0], scene_bounds_max[1], scene_bounds_max[2]],\n",
    "        [scene_bounds_max[0], scene_bounds_min[1], scene_bounds_min[2]],\n",
    "        [scene_bounds_max[0], scene_bounds_min[1], scene_bounds_max[2]],\n",
    "        [scene_bounds_max[0], scene_bounds_max[1], scene_bounds_min[2]],\n",
    "        [scene_bounds_max[0], scene_bounds_max[1], scene_bounds_max[2]],\n",
    "    ])\n",
    "    \n",
    "    nears, fars = [], []\n",
    "    for c2w in c2ws:\n",
    "        corners_camera = (c2w[:3, :3] @ corners.T).T + c2w[:3, -1]\n",
    "        distances = torch.norm(corners_camera, \"fro\", dim=1)\n",
    "        nears.append(torch.min(distances))\n",
    "        fars.append(torch.max(distances))\n",
    "    \n",
    "    near_plane = min(distances) * 0.9  # Slightly smaller than the closest point\n",
    "    far_plane = max(distances) * 1.1  # Slightly larger than the farthest point\n",
    "\n",
    "    return near_plane.item(), far_plane.item()\n",
    "\n",
    "near, far = compute_near_far_planes(c2ws)\n",
    "print(f\"Near and far planes: {near=} | {far=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "@torch.no_grad()\n",
    "def sobel_filter(images: Tensor):\n",
    "    # Sobel-Feldman operator\n",
    "    filter = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=1, padding_mode='zeros', bias=False, dtype=torch.float32)\n",
    "    gx = torch.tensor([\n",
    "        [3.0, 0.0, -3.0],\n",
    "        [10.0, 0.0, -10.0],\n",
    "        [3.0, 0.0, -3.0],\n",
    "    ], dtype=torch.float32)\n",
    "    gy = torch.tensor([\n",
    "        [3.0,  10.0,  3.0],\n",
    "        [0.0,  0.0,  0.0],\n",
    "        [-3.0, -10.0, -3.0],\n",
    "    ], dtype=torch.float32)\n",
    "    weights = torch.stack([gx, gy], 0).unsqueeze(1)\n",
    "    filter.weight = nn.Parameter(weights, requires_grad=False)\n",
    "\n",
    "    edges = filter(images.mean(dim=-1).unsqueeze(1))\n",
    "    edges = torch.sqrt(torch.sum(edges ** 2, dim=1))\n",
    "    return edges\n",
    "\n",
    "edges = sobel_filter(images)\n",
    "plt.hist(edges.flatten()[edges.flatten() != 0], bins=30)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(edges[5], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: DATAMODULE\n",
    "def create_nerf_data(images: Tensor, c2ws: Tensor, focal: Tensor, weight_epsilon: float = 0.33):\n",
    "    origins = []\n",
    "    directions = []\n",
    "    colors = []\n",
    "\n",
    "    # Collecting to list then concat for ease\n",
    "    for image, c2w in zip(images, c2ws):\n",
    "        intrinsic = torch.tensor([\n",
    "            [focal.item(), 0, image.shape[1] // 2],\n",
    "            [0, focal.item(), image.shape[0] // 2],\n",
    "            [0, 0, 1],\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        o, d = create_rays(image.shape[0], image.shape[1], intrinsic, c2w)\n",
    "\n",
    "        origins.append(o.flatten(0, 1))\n",
    "        directions.append(d.flatten(0, 1))\n",
    "        colors.append(image.flatten(0, 1))\n",
    "\n",
    "    origins = torch.cat(origins, dim=0)\n",
    "    directions = torch.cat(directions, dim=0)\n",
    "    colors = torch.cat(colors, dim=0)\n",
    "\n",
    "    pixel_weights = sobel_filter(images).flatten() + weight_epsilon\n",
    "    return origins, directions, colors, pixel_weights\n",
    "\n",
    "\n",
    "origins, directions, colors, pixel_weights = create_nerf_data(images, c2ws, focal)\n",
    "\n",
    "print(f\"{origins.shape=} | {directions.shape=} | {colors.shape=}\")\n",
    "print(f\"Training origins' size:    {origins.element_size() * origins.nelement() / (1024**2):.2f} MB\")\n",
    "print(f\"Training directions' size: {directions.element_size() * directions.nelement() / (1024**2):.2f} MB\")\n",
    "print(f\"Training images' size:     {colors.element_size() * colors.nelement() / (1024**2):.2f} MB\")\n",
    "print(f\"Pixel weights' size:       {pixel_weights.element_size() * pixel_weights.nelement() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE, used inside DATAMODULE\n",
    "class ImportantPixelSampler(WeightedRandomSampler):\n",
    "    def __init__(self, weights: Tensor, num_samples: int, replacement: bool = True, swap_strategy_iter: int = 100):\n",
    "        super(ImportantPixelSampler, self).__init__(weights=weights, num_samples=num_samples, replacement=replacement, generator=None)\n",
    "        weights = weights.to(torch.float32)\n",
    "        self.pixel_weights = weights / weights.max()\n",
    "        self.weights = self.pixel_weights\n",
    "        self.swap_strategy_iter = swap_strategy_iter\n",
    "        self.num_iters = 0\n",
    "        self.squared_errors = torch.ones(self.pixel_weights.shape, dtype=torch.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.num_iters += 1\n",
    "        yield from super(ImportantPixelSampler, self).__iter__()\n",
    "\n",
    "    def update_errors(self, idxs: Tensor, errors: Tensor):\n",
    "        errors = errors.clone().cpu().detach()\n",
    "        # Epsilon evaluates to 5e-3 as the prev value contributes 20%\n",
    "        self.squared_errors[idxs] = self.squared_errors[idxs] * 0.2 + errors * 0.8  + 4e-3  # Discounted error update\n",
    "        pxw = torch.clamp(torch.tensor([1.0], dtype=torch.float32) - (self.num_iters) / self.swap_strategy_iter, 0.0, 1.0)\n",
    "        self.weights[idxs] = self.pixel_weights[idxs] * pxw + self.squared_errors[idxs] * (1 - pxw)\n",
    "\n",
    "\n",
    "# Samplers with DataLoaders: Sampler num_samples decides dataloader \"length\" (like how Dataset length usually works)\n",
    "# DataLoader batch size decided the batch size as per usual\n",
    "sampler = ImportantPixelSampler(pixel_weights, 16, False)\n",
    "for _ in range(10):\n",
    "    for sample in sampler:\n",
    "        pass\n",
    "    sampler.update_errors(torch.arange(origins.shape[0]), torch.tensor([2.0], dtype=torch.float32))\n",
    "\n",
    "\n",
    "for sample in sampler:\n",
    "    print(f\"Sample: {sample:07d} | Weight: {sampler.weights[sample]:5.2f}\")\n",
    "\n",
    "ex_data = TensorDataset(origins, directions, colors)\n",
    "ex_loader = DataLoader(ex_data, batch_size=4, sampler=sampler)\n",
    "for o, d, c in ex_loader:\n",
    "    print(o.shape, d.shape, c.shape)\n",
    "\n",
    "ex_loader.sampler.weights\n",
    "sampler.num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Appended Positional Encoding Module\n",
    "\n",
    "    def __init__(self, max_freq: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._max_freq = max_freq\n",
    "        freq_bands = 2.0 ** torch.linspace(0.0, max_freq - 1, steps=max_freq, dtype=torch.float32)\n",
    "        self._freq_bands = nn.parameter.Buffer(freq_bands)\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        encs = (x[..., None] * self._freq_bands).flatten(-2, -1)\n",
    "        # Encoding to (x, sin parts, cos parts) of shape(N, M+M*max_freq*2) if x is of shape(N,M)\n",
    "        return torch.cat([x, encs.sin(), encs.cos()], dim=-1)\n",
    "    \n",
    "    def get_out_dim(self, in_dim: int):\n",
    "        return in_dim + in_dim * self._max_freq * 2\n",
    "    \n",
    "    \n",
    "enc = PositionalEncoding(10).to(COMPUTE_DEVICE)\n",
    "print(f\"Output dimension: {enc.get_out_dim(3)}\")\n",
    "enc(origins[:1000, :].to(COMPUTE_DEVICE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def sample_ray_uniformally(origins: Tensor, directions: Tensor, near: float, far: float, num_samples: int, perturb=True):\n",
    "    device = origins.device\n",
    "    depths = torch.linspace(near, far, num_samples, dtype=torch.float32, device=device).expand(origins.shape[0], -1)\n",
    "\n",
    "    if perturb:\n",
    "        # Noise is at most half of step size, this ensures sorted depths, required for volume rendering\n",
    "        noise = (torch.rand(depths.shape, device=device) - 0.5) * (far - near) / num_samples / 2\n",
    "        # Clamping to stay between near and far\n",
    "        depths = (depths + noise).clamp(near, far)\n",
    "\n",
    "    points = origins[..., None, :] + directions[..., None, :] * depths[..., :, None]\n",
    "    # Expand directions to make NeRF input\n",
    "    directions = directions[..., None, :].expand(points.shape)\n",
    "    return points, directions, depths\n",
    "\n",
    "\n",
    "# Lightning conversion proposal: STANDALONE\n",
    "def plot_ray_sampling(points: Tensor, origin: Tensor, cartesian_direction: Tensor, title: str):\n",
    "    points = points.cpu()\n",
    "    origin = origin.cpu()\n",
    "    cartesian_direction = cartesian_direction.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,8), subplot_kw={\"projection\": \"3d\"})\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    # Adding the origin so it always starts from the camera position\n",
    "    points = torch.cat([origin.expand((points.shape[0], 1, -1)), points], 1)\n",
    "\n",
    "    # Convert to spherical coordinates\n",
    "    X, Y, Z = -cartesian_direction  # Taking the negative as view_init specifies direction outward\n",
    "    R = torch.sqrt(X**2 + Y**2 + Z**2)\n",
    "    X, Y, Z = X/R, Y/R, Z/R  # normalization\n",
    "    azim = torch.rad2deg(torch.atan2(Y, X))\n",
    "    elev = torch.rad2deg(torch.arcsin(Z))\n",
    "    # Multiple angles to understand better\n",
    "    for ax, (mod_elev, mod_azim) in zip(axes, [[0,0],[-10, 60]]):\n",
    "        ax.view_init(elev + mod_elev, azim + mod_azim, 0)\n",
    "        ax.plot(points[:, :, 0], points[:, :, 1], points[:, :, 2], linewidth=0.2, markersize=2, marker='o')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "points, dirs, depths = sample_ray_uniformally(\n",
    "    origins[:10000:71].to(COMPUTE_DEVICE),\n",
    "    directions[:10000:71].to(COMPUTE_DEVICE),\n",
    "    near, far, 7, perturb=True\n",
    ")\n",
    "\n",
    "print(f\"{points.shape=} | {dirs.shape=} | {depths.shape=}\")\n",
    "plot_ray_sampling(points, origins[0], dirs[:, 0, :].mean(0), \"Example of perturbed uniform samples along ray\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def sample_pdf(bins: Tensor, weights: Tensor, num_samples: int, deterministic: bool = False):\n",
    "    device = weights.device\n",
    "\n",
    "    weights = weights + 1e-5  # avoid nans later\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)  # Normalize PDF\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[..., :1], device=device), cdf], dim=-1)  # Prepend 0 to have cdf->[0,1]\n",
    "\n",
    "    if deterministic:\n",
    "        u = torch.linspace(0.0, 1.0, steps=num_samples, device=device)\n",
    "        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [num_samples], device=device)\n",
    "\n",
    "    # Inverting the CDF\n",
    "    u = u.contiguous()  # Need contigous memory layout for further operations\n",
    "    indexes = torch.searchsorted(cdf, u, right=True)  # Finding bins\n",
    "    # Need to ensure below and above don't leave bounds of bins\n",
    "    below = torch.max(torch.zeros_like(indexes-1), indexes-1)\n",
    "    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(indexes), indexes)\n",
    "    indexes = torch.stack([below, above], dim=-1)\n",
    "\n",
    "    # Gathering sampled bins and bound probabilites\n",
    "    shape = [indexes.shape[0], indexes.shape[1], cdf.shape[-1]]\n",
    "    cdf = torch.gather(cdf.unsqueeze(1).expand(shape), dim=2, index=indexes)\n",
    "    bins = torch.gather(bins.unsqueeze(1).expand(shape), dim=2, index=indexes)\n",
    "\n",
    "    # denominator is the size of the bins\n",
    "    denominator = cdf[..., 1] - cdf[..., 0]\n",
    "    denominator = torch.where(denominator < 1e-5, torch.ones_like(denominator), denominator)\n",
    "    denominator[denominator < 1e-5] = 1.0\n",
    "    # t gives the relative position inside the bins \n",
    "    t = (u - cdf[..., 0]) / denominator\n",
    "\n",
    "    samples = bins[..., 0] + t * (bins[..., 1] - bins[..., 0])\n",
    "    return samples\n",
    "\n",
    "\n",
    "bins = torch.linspace(0, 1, steps=11) # 10 intervals (bins)\n",
    "weights = torch.tensor([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.3, 0.25, 0.2, 0.1]) # Arbitrary weights\n",
    "weights = weights / torch.sum(weights, -1)\n",
    "\n",
    "# Expand the inputs to include a batch dimension\n",
    "bins = bins.unsqueeze(0).to(COMPUTE_DEVICE) # Shape: [1, 11]\n",
    "weights = weights.unsqueeze(0).to(COMPUTE_DEVICE) # Shape: [1, 10]\n",
    "samples = sample_pdf(bins, weights, 5)\n",
    "\n",
    "print(f\"{samples=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def sample_ray_hierarchically(origins: Tensor, directions: Tensor, num_samples: int, bins: Tensor, weights: Tensor, deterministic: bool = False):\n",
    "    depths = sample_pdf(bins, weights, num_samples, deterministic=deterministic)\n",
    "\n",
    "    points = origins[..., None, :] + directions[..., None, :] * depths[..., :, None]\n",
    "    # Expand directions to make NeRF input\n",
    "    directions = directions[..., None, :].expand(points.shape)\n",
    "    return points, directions, depths\n",
    "\n",
    "bins = torch.linspace(0, 1, 65).expand((10000 // 41 + 1, 65))  # Computed after uniform sampling\n",
    "weights = torch.randn((10000 // 41 + 1, 64))  # Retrieved from NeRF's sigma values\n",
    "points, dirs, depths = sample_ray_hierarchically(\n",
    "    origins[:10000:41].to(COMPUTE_DEVICE),\n",
    "    directions[:10000:41].to(COMPUTE_DEVICE),\n",
    "    32, bins.to(COMPUTE_DEVICE), weights.to(COMPUTE_DEVICE)\n",
    ")\n",
    "\n",
    "print(f\"{points.shape=} | {dirs.shape=} | {depths.shape=}\")\n",
    "plot_ray_sampling(points, origins[0], dirs[:, 0, :].mean(0), \"Example of hierarchical samples along ray\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: NeRF() STANDALONE, compute_along_rays LIGHTNING MODULE\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, num_layers: int = 8, hidden_size: int = 256, in_coordinates: int = 3, in_directions: int = 3,\n",
    "                 skips: list[int] = [4], coord_encode_freq: int = 10, dir_encode_freq: int = 4):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.in_coordinates = in_coordinates\n",
    "        self.in_directions = in_directions\n",
    "        self.skips = tuple(skips)\n",
    "\n",
    "        self.coordinate_encoder = PositionalEncoding(coord_encode_freq)\n",
    "        self.direction_encoder = PositionalEncoding(dir_encode_freq)\n",
    "\n",
    "        coord_dim = self.coordinate_encoder.get_out_dim(self.in_coordinates)\n",
    "        self.feature_mlp = nn.ModuleList([nn.Linear(coord_dim, hidden_size)])\n",
    "        # go until num_layers -1 as we already have the initial layer\n",
    "        for i in range(num_layers - 1):\n",
    "            self.feature_mlp.append(nn.Sequential(\n",
    "                # skip with +1 as we already have the initial layer\n",
    "                nn.Linear(hidden_size + (coord_dim if i+1 in self.skips else 0), hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ))\n",
    "\n",
    "        self.sigma_fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.color_preproc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        dir_dim = self.direction_encoder.get_out_dim(self.in_directions)\n",
    "        self.rgb_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size + dir_dim, hidden_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size // 2, 3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, coordinates: Tensor, directions: Tensor, skip_colors: bool = False):\n",
    "        coordinates = self.coordinate_encoder(coordinates)\n",
    "        features = coordinates\n",
    "        for i, fc in enumerate(self.feature_mlp):\n",
    "            if i in self.skips:\n",
    "                features = torch.cat([features, coordinates], -1)\n",
    "            features = fc(features)\n",
    "\n",
    "        sigma = self.sigma_fc(features)\n",
    "\n",
    "        if skip_colors:\n",
    "            return sigma\n",
    "\n",
    "        directions = self.direction_encoder(directions)\n",
    "        features = self.color_preproc(features)\n",
    "        features = torch.cat([features, directions], dim=-1)\n",
    "        rgb = self.rgb_mlp(features)\n",
    "\n",
    "        return torch.cat([rgb, sigma], dim=-1)\n",
    "    \n",
    "    def compute_along_rays(self, origins: Tensor, directions: Tensor, near: float, far: float,\n",
    "                           coarse_samples: int, fine_samples: int, deterministic: bool = True):\n",
    "        device = origins.device\n",
    "        # This function deviates from the original NeRF paper as the coarse and fine samples are processed by the same model\n",
    "        points, expanded_directions, coarse_depths = sample_ray_uniformally(origins, directions, near, far, coarse_samples)\n",
    "        coarse_out = self(points, expanded_directions)\n",
    "        \n",
    "        # Bin bounds are halfway between sampled coordinates + near + far plane\n",
    "        bins = torch.cat([\n",
    "            torch.tensor(near, dtype=torch.float32, device=device).expand(origins.shape[0], 1),\n",
    "            (coarse_depths[..., 1:] + coarse_depths[..., :-1]) / 2,\n",
    "            torch.tensor(far, dtype=torch.float32, device=device).expand(origins.shape[0], 1),\n",
    "        ], -1)\n",
    "\n",
    "        points, expanded_directions, fine_depths = sample_ray_hierarchically(origins, directions, fine_samples, bins,\n",
    "                                                                             coarse_out[..., -1], deterministic=deterministic)\n",
    "        fine_out = self(points, expanded_directions)\n",
    "\n",
    "        # deterministic ensures depth sorted output, if non-deterministic, sort manually as sortedness is required for volume rendering\n",
    "        if not deterministic:\n",
    "            fine_depths, idxs = torch.sort(fine_depths, dim=-1)\n",
    "            fine_out = fine_out[torch.arange(idxs.shape[0]).unsqueeze(1), idxs]\n",
    "\n",
    "        return coarse_out, coarse_depths, fine_out, fine_depths\n",
    "    \n",
    "\n",
    "model = NeRF().to(COMPUTE_DEVICE)\n",
    "print(f\"Output shape for origins + directions: {model(origins[:50].to(COMPUTE_DEVICE), directions[:50].to(COMPUTE_DEVICE)).shape}\")\n",
    "\n",
    "coarse_rgbs, coarse_depths, fine_rgbs, fine_depths = model.compute_along_rays(\n",
    "    origins[:500].to(COMPUTE_DEVICE),\n",
    "    directions[:500].to(COMPUTE_DEVICE),\n",
    "    near, far, 64, 5, False\n",
    ")\n",
    "print(f\"Coarse output shape for hierarchical sampling: {coarse_rgbs.shape=} | {coarse_depths.shape=}\")\n",
    "print(f\"Fine output shape for hierarchical sampling:   {fine_rgbs.shape=} | {fine_depths.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def render_rays(rgbs: Tensor, depths: Tensor):\n",
    "    device = rgbs.device\n",
    "\n",
    "    distances = depths[..., 1:] - depths[..., :-1]\n",
    "    # 1e10 ensures the last color is rendered no matter what\n",
    "    distances = torch.cat([distances, torch.tensor([1e10], device=device).expand(distances[...,:1].shape)], -1)\n",
    "    # directions already normalized at ray calculation, so distances correspond to world already\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-F.relu(rgbs[..., 3]) * distances)\n",
    "    # 1e10 ensures the last color is rendered no matter what\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1), device=device), 1. - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb = torch.sum(weights[..., None] * rgbs[..., :3], dim=-2)\n",
    "    depth = torch.sum(weights * depths, dim=-1)\n",
    "\n",
    "    return rgb, depth\n",
    "\n",
    "rendered_rgb, rendered_depth = render_rays(coarse_rgbs, coarse_depths)\n",
    "print(f\"Color and depth shapes after render: {rendered_rgb.shape=} | {rendered_depth.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: LIGHTNING MODULE\n",
    "@torch.no_grad()\n",
    "def render_image(model: NeRF, height: int, width: int, c2w: Tensor, focal: Tensor,\n",
    "                 near: float, far: float, batch_size=512):\n",
    "    device = next(iter(model.parameters())).device\n",
    "    \n",
    "    intrinsic = torch.tensor([\n",
    "        [focal.item(), 0, width // 2],\n",
    "        [0, focal.item(), height // 2],\n",
    "        [0, 0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "    origins, directions = create_rays(height, width, intrinsic, c2w)\n",
    "\n",
    "    origins = origins.to(device).flatten(0, -2)\n",
    "    directions = directions.to(device).flatten(0, -2)\n",
    "    data = TensorDataset(origins, directions)\n",
    "    data = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    image = []\n",
    "    for o, d in data:\n",
    "        _, _, rgbs, depths = model.compute_along_rays(o, d, near, far, 64, 64)\n",
    "        rgb, _ = render_rays(rgbs, depths)\n",
    "        image.append(rgb)\n",
    "\n",
    "    image = torch.cat(image, 0).reshape(height, width, -1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "render = render_image(NeRF().to(COMPUTE_DEVICE), 50, 50, c2ws[-1], focal, near, far).cpu()\n",
    "plt.imshow(render)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.load(\"../data/tiny_nerf_data.npz\")\n",
    "images, c2ws, focal = torch.from_numpy(data[\"images\"]), torch.from_numpy(data[\"poses\"]), torch.from_numpy(data[\"focal\"])\n",
    "near, far = compute_near_far_planes(c2ws)\n",
    "\n",
    "test_img, test_c2w = images[-5].unsqueeze(0), c2ws[-5].unsqueeze(0)\n",
    "train_imgs, train_c2ws = torch.cat([images[:-5], images[-4:]], 0), torch.cat([images[:-5], images[-4:]], 0)\n",
    "train_origins, train_directions, train_colors, pixel_weights = create_nerf_data(images, c2ws, focal, weight_epsilon=0.33)\n",
    "\n",
    "print(f\"{train_origins.shape=} | {train_directions.shape=} | {train_colors.shape=} | {pixel_weights.shape=}\")\n",
    "\n",
    "plt.title(\"Testing image\")\n",
    "plt.imshow(test_img[0])\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "# Batch size determines iteration per epoch, swap strategy is based on epochs\n",
    "train_sampler = ImportantPixelSampler(pixel_weights, batch_size * 20, False, 400)\n",
    "train_data = TensorDataset(torch.arange(train_origins.shape[0]), train_origins, train_directions, train_colors)\n",
    "train_loader = DataLoader(train_data, batch_size, sampler=train_sampler, num_workers=2, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: NeRF = NeRF().to(COMPUTE_DEVICE)\n",
    "print(f\"Trainable params count: {sum(p.numel() for p in model.parameters() if p.requires_grad):_}\")\n",
    "\n",
    "coarse_samples = 64\n",
    "fine_samples = 64\n",
    "epochs = 5000\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_func = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# RTX 3050Ti Max-Q, Ryzen 5 5600H => ~4.3s/it at batch_size=1024, num_samples=batch_size*20\n",
    "with tqdm(range(1, epochs + 1), desc=f\"Epoch\", position=0, leave=True) as epoch_progress:\n",
    "    for epoch in epoch_progress:\n",
    "\n",
    "        inter_train_losses = []\n",
    "        for i, origins, directions, colors in train_loader:\n",
    "            origins = origins.to(COMPUTE_DEVICE)\n",
    "            directions = directions.to(COMPUTE_DEVICE)\n",
    "            colors = colors.to(COMPUTE_DEVICE)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            coarse_rgbs, coarse_depths, fine_rgbs, fine_depths = model.compute_along_rays(origins, directions, near, far, coarse_samples, fine_samples)\n",
    "\n",
    "            coarse_colors, _ = render_rays(coarse_rgbs, coarse_depths)\n",
    "            fine_colors, _ = render_rays(fine_rgbs, fine_depths)\n",
    "\n",
    "            loss = loss_func(coarse_colors, colors) + loss_func(fine_colors, colors)\n",
    "            loss = loss.mean(-1)\n",
    "            train_loader.sampler.update_errors(i, loss)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            inter_train_losses.append(loss.item() * colors.shape[0])\n",
    "        \n",
    "        train_losses.append(sum(inter_train_losses) / train_sampler.num_samples)\n",
    "        epoch_progress.set_postfix(loss=train_losses[-1])\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            render = render_image(model, 100, 100, test_c2w.squeeze(0), focal, near, far)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12,4), width_ratios=(0.4, 0.6))\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            axes[0].set_title(f\"Render at epoch {epoch}\")\n",
    "            axes[0].imshow(render.cpu())\n",
    "\n",
    "            axes[1].set_title(f\"Losses up to epoch {epoch}\")\n",
    "            axes[1].plot(train_losses)\n",
    "            axes[1].set_ylim(bottom=0.0, top=0.35)\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"tiny_lego_nerf.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
