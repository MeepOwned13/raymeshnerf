{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow importing from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.load(\"../data/tiny_nerf_data.npz\")\n",
    "images, c2ws, focal = data[\"images\"], data[\"poses\"], data[\"focal\"]\n",
    "\n",
    "print(\n",
    "    f\"Shapes:\",\n",
    "    f\"{images.shape=}\",\n",
    "    f\"{c2ws.shape=}\",\n",
    "    f\"{focal.shape=}\",\n",
    "    sep='\\n  ',\n",
    ")\n",
    "\n",
    "plt.imshow(images[2])\n",
    "plt.title(\"Image at index 2\")\n",
    "\n",
    "print(f\"C2W transform at index 2: \\n\", c2ws[2])\n",
    "\n",
    "print(f\"Focal length: {focal:.4f}\")\n",
    "\n",
    "images = torch.from_numpy(images)\n",
    "c2ws = torch.from_numpy(c2ws)\n",
    "focal = torch.from_numpy(focal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "To be exported into `ROOT_DIR/src/utils/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rays(height, width, intrinsic, c2w):\n",
    "    focal_x = intrinsic[0, 0]\n",
    "    focal_y = intrinsic[1, 1]\n",
    "    # cx and cy handle the misalignement of the principal point with the center of the image\n",
    "    cx = intrinsic[0, 2]\n",
    "    cy = intrinsic[1, 2]\n",
    "\n",
    "    # Index each point on the image, determine ray directions to them\n",
    "    i, j = torch.meshgrid(torch.arange(width, dtype=torch.float32), torch.arange(height, dtype=torch.float32), indexing='xy')\n",
    "    directions = torch.stack((\n",
    "        (i - cx) / focal_x,\n",
    "        -(j - cy) / focal_y,\n",
    "        -torch.ones(i.shape, dtype=torch.float32)  # -1 since ray is cast away from camera\n",
    "    ), -1)\n",
    "\n",
    "    # Transform ray directions to World, origins just need to be broadcasted accordingly\n",
    "    ray_directions = directions @ c2w[:3, :3].T\n",
    "    ray_origins = torch.broadcast_to(c2w[:3, -1], ray_directions.shape)  # c2w last column determines position\n",
    "    \n",
    "    return ray_origins, ray_directions\n",
    "\n",
    "\n",
    "# Test on real data\n",
    "ex_index = 2\n",
    "ex_img = images[ex_index]\n",
    "ex_intr = torch.tensor([\n",
    "    [focal.item(), 0, ex_img.shape[1] // 2],\n",
    "    [0, focal.item(), ex_img.shape[0] // 2],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "origins, directions = create_rays(ex_img.shape[0], ex_img.shape[1], ex_intr, c2ws[ex_index])\n",
    "\n",
    "\"\"\"\n",
    "# Test on example data\n",
    "ex_intr = torch.tensor([\n",
    "    [4, 0, 5 // 2],\n",
    "    [0, 4, 5 // 2],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "ex_c2w = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "o, d = create_rays(5, 5, ex_intr, ex_c2w)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"{origins.shape=} | {directions.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nerf_data(images, c2ws, focal):\n",
    "    rays = []\n",
    "    colors = []\n",
    "\n",
    "    # Collecting to list then concat for ease\n",
    "    for image, c2w in zip(images, c2ws):\n",
    "        intrinsic = torch.tensor([\n",
    "            [focal.item(), 0, image.shape[1] // 2],\n",
    "            [0, focal.item(), image.shape[0] // 2],\n",
    "            [0, 0, 1],\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        origins, directions = create_rays(image.shape[0], image.shape[1], intrinsic, c2w)\n",
    "\n",
    "        data = torch.stack([origins, directions], dim=2)\n",
    "        rays.append(data.flatten(0, 1))\n",
    "        colors.append(image.flatten(0, 1))\n",
    "        # shape(height * width, 1+1+1, 3) {1+1+1===origin, direction, rgb}\n",
    "\n",
    "    rays = torch.cat(rays, dim=0)\n",
    "    colors = torch.cat(colors, dim=0)\n",
    "    return rays, colors\n",
    "\n",
    "rays, colors = create_nerf_data(images, c2ws, focal)\n",
    "\n",
    "print(f\"{rays.shape=} | {colors.shape=}\")\n",
    "print(f\"Training rays' size:   {rays.element_size() * rays.nelement() / (1024**2):.2f} MB\")\n",
    "print(f\"Training images' size: {colors.element_size() * colors.nelement() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # Appended Positional Encoding Module\n",
    "\n",
    "    def __init__(self, max_freq: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._max_freq = max_freq\n",
    "        self._freq_bands = 2.0 ** torch.linspace(0.0, max_freq - 1, steps=max_freq, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encs = (x[..., None] * self._freq_bands).flatten(-2, -1)\n",
    "        # Encoding to (x, sin parts, cos parts) of shape(N, M+M*max_freq*2) if x is of shape(N,M)\n",
    "        return torch.cat([x, encs.sin(), encs.cos()], dim=-1)\n",
    "    \n",
    "    def get_out_dim(self, in_dim):\n",
    "        return in_dim + in_dim * self._max_freq * 2\n",
    "    \n",
    "enc = PositionalEncoding(10)\n",
    "print(f\"Output dimension: {enc.get_out_dim(3)}\")\n",
    "enc(rays[:1000, 1, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self, num_layers=8, hidden_size=256, in_coordinates=3, in_directions=3,\n",
    "                 skips=[4], coord_encode_freq=10, dir_encode_freq=4):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.in_coordinates = in_coordinates\n",
    "        self.in_directions = in_directions\n",
    "        self.skips = tuple(skips)\n",
    "\n",
    "        self.coordinate_encoder = PositionalEncoding(coord_encode_freq)\n",
    "        self.direction_encoder = PositionalEncoding(dir_encode_freq)\n",
    "\n",
    "        coord_dim = self.coordinate_encoder.get_out_dim(self.in_coordinates)\n",
    "        self.feature_mlp = nn.ModuleList([nn.Linear(coord_dim, hidden_size)])\n",
    "        # go until num_layers -1 as we already have the initial layer\n",
    "        for i in range(num_layers - 1):\n",
    "            self.feature_mlp.append(nn.Sequential(\n",
    "                # skip with +1 as we already have the initial layer\n",
    "                nn.Linear(hidden_size + (coord_dim if i+1 in self.skips else 0), hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ))\n",
    "\n",
    "        self.sigma_fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.color_preproc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        dir_dim = self.direction_encoder.get_out_dim(self.in_directions)\n",
    "        self.rgb_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size + dir_dim, hidden_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size // 2, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        coordinates, directions = torch.split(x, [self.in_coordinates, self.in_directions], -1)\n",
    "        coordinates = self.coordinate_encoder(coordinates)\n",
    "\n",
    "        features = coordinates\n",
    "        for i, fc in enumerate(self.feature_mlp):\n",
    "            if i in self.skips:\n",
    "                features = torch.cat([features, coordinates], -1)\n",
    "            features = fc(features)\n",
    "\n",
    "        sigma = self.sigma_fc(features)\n",
    "\n",
    "        directions = self.direction_encoder(directions)\n",
    "        rgb = self.color_preproc(features)\n",
    "        rgb = torch.cat([rgb, directions], dim=-1)\n",
    "        rgb = self.rgb_mlp(rgb)\n",
    "\n",
    "        return torch.cat([rgb, sigma], dim=-1)\n",
    "    \n",
    "\n",
    "NeRF()(rays[:139].flatten(-2, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
