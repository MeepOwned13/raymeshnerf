{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow importing from src\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "COMPUTE_DEVICE = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    COMPUTE_DEVICE = torch.device('cuda:0')\n",
    "elif torch.mps.is_available():\n",
    "    COMPUTE_DEVICE = torch.device('mps')\n",
    "print(f\"{COMPUTE_DEVICE=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.load(\"../data/tiny_nerf_data.npz\")\n",
    "images, c2ws, focal = data[\"images\"], data[\"poses\"], data[\"focal\"]\n",
    "\n",
    "print(\n",
    "    f\"Shapes:\",\n",
    "    f\"{images.shape=}\",\n",
    "    f\"{c2ws.shape=}\",\n",
    "    f\"{focal.shape=}\",\n",
    "    sep='\\n  ',\n",
    ")\n",
    "\n",
    "plt.imshow(images[2])\n",
    "plt.title(\"Image at index 2\")\n",
    "\n",
    "print(f\"C2W transform at index 2: \\n\", c2ws[2])\n",
    "\n",
    "print(f\"Focal length: {focal:.4f}\")\n",
    "\n",
    "images = torch.from_numpy(images)\n",
    "c2ws = torch.from_numpy(c2ws)\n",
    "focal = torch.from_numpy(focal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "To be exported into `ROOT_DIR/src/utils/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def create_rays(height, width, intrinsic, c2w):\n",
    "    focal_x = intrinsic[0, 0]\n",
    "    focal_y = intrinsic[1, 1]\n",
    "    # cx and cy handle the misalignement of the principal point with the center of the image\n",
    "    cx = intrinsic[0, 2]\n",
    "    cy = intrinsic[1, 2]\n",
    "\n",
    "    # Index each point on the image, determine ray directions to them\n",
    "    i, j = torch.meshgrid(torch.arange(width, dtype=torch.float32), torch.arange(height, dtype=torch.float32), indexing='xy')\n",
    "    directions = torch.stack((\n",
    "        (i - cx) / focal_x,\n",
    "        -(j - cy) / focal_y,\n",
    "        -torch.ones(i.shape, dtype=torch.float32)  # -1 since ray is cast away from camera\n",
    "    ), -1)\n",
    "\n",
    "    # Transform ray directions to World, origins just need to be broadcasted accordingly\n",
    "    ray_directions = F.normalize(directions @ c2w[:3, :3].T, \"fro\", -1)\n",
    "    ray_origins = torch.broadcast_to(c2w[:3, -1], ray_directions.shape)  # c2w last column determines position\n",
    "    \n",
    "    return ray_origins, ray_directions\n",
    "\n",
    "\n",
    "# Test on real data\n",
    "ex_index = 2\n",
    "ex_img = images[ex_index]\n",
    "ex_intr = torch.tensor([\n",
    "    [focal.item(), 0, ex_img.shape[1] // 2],\n",
    "    [0, focal.item(), ex_img.shape[0] // 2],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "origins, directions = create_rays(ex_img.shape[0], ex_img.shape[1], ex_intr, c2ws[ex_index])\n",
    "\n",
    "\"\"\"\n",
    "# Test on example data\n",
    "ex_intr = torch.tensor([\n",
    "    [4, 0, 5 // 2],\n",
    "    [0, 4, 5 // 2],\n",
    "    [0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "ex_c2w = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 0, 0, 1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "o, d = create_rays(5, 5, ex_intr, ex_c2w)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"{origins.shape=} | {directions.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: DATAMODULE\n",
    "def create_nerf_data(images, c2ws, focal):\n",
    "    rays = []\n",
    "    colors = []\n",
    "\n",
    "    # Collecting to list then concat for ease\n",
    "    for image, c2w in zip(images, c2ws):\n",
    "        intrinsic = torch.tensor([\n",
    "            [focal.item(), 0, image.shape[1] // 2],\n",
    "            [0, focal.item(), image.shape[0] // 2],\n",
    "            [0, 0, 1],\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        origins, directions = create_rays(image.shape[0], image.shape[1], intrinsic, c2w)\n",
    "\n",
    "        data = torch.stack([origins, directions], dim=2)\n",
    "        rays.append(data.flatten(0, 1))\n",
    "        colors.append(image.flatten(0, 1))\n",
    "        # shape(height * width, 1+1+1, 3) {1+1+1===origin, direction, rgb}\n",
    "\n",
    "    rays = torch.cat(rays, dim=0)\n",
    "    colors = torch.cat(colors, dim=0)\n",
    "    return rays, colors\n",
    "\n",
    "\n",
    "rays, colors = create_nerf_data(images, c2ws, focal)\n",
    "\n",
    "print(f\"{rays.shape=} | {colors.shape=}\")\n",
    "print(f\"Training rays' size:   {rays.element_size() * rays.nelement() / (1024**2):.2f} MB\")\n",
    "print(f\"Training images' size: {colors.element_size() * colors.nelement() / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Appended Positional Encoding Module\n",
    "\n",
    "    def __init__(self, max_freq: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._max_freq = max_freq\n",
    "        freq_bands = 2.0 ** torch.linspace(0.0, max_freq - 1, steps=max_freq, dtype=torch.float32)\n",
    "        self._freq_bands = nn.parameter.Buffer(freq_bands)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encs = (x[..., None] * self._freq_bands).flatten(-2, -1)\n",
    "        # Encoding to (x, sin parts, cos parts) of shape(N, M+M*max_freq*2) if x is of shape(N,M)\n",
    "        return torch.cat([x, encs.sin(), encs.cos()], dim=-1)\n",
    "    \n",
    "    def get_out_dim(self, in_dim):\n",
    "        return in_dim + in_dim * self._max_freq * 2\n",
    "    \n",
    "    \n",
    "enc = PositionalEncoding(10).to(COMPUTE_DEVICE)\n",
    "print(f\"Output dimension: {enc.get_out_dim(3)}\")\n",
    "enc(rays[:1000, 1, :].to(COMPUTE_DEVICE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def sample_ray_uniformally(origins, directions, near, far, num_samples, perturb=True):\n",
    "    depths = torch.linspace(near, far, num_samples, dtype=torch.float32).expand(origins.shape[0], -1)\n",
    "\n",
    "    if perturb:\n",
    "        # Noise is at most half of step size, this ensures sorted depths, required for volume rendering\n",
    "        noise = (torch.rand(depths.shape) - 0.5) * (far - near) / num_samples / 2\n",
    "        # Clamping to stay between near and far\n",
    "        depths = (depths + noise).clamp(near, far)\n",
    "\n",
    "    points = origins[..., None, :] + directions[..., None, :] * depths[..., :, None]\n",
    "    # Expand directions to make NeRF input\n",
    "    directions = directions[..., None, :].expand(points.shape)\n",
    "    return points, directions, depths\n",
    "\n",
    "\n",
    "# Lightning conversion proposal: STANDALONE\n",
    "def plot_ray_sampling(points, origin, cartesian_direction, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,8), subplot_kw={\"projection\": \"3d\"})\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    # Adding the origin so it always starts from the camera position\n",
    "    points = torch.cat([origin.expand((points.shape[0], 1, -1)), points], 1)\n",
    "\n",
    "    # Convert to spherical coordinates\n",
    "    X, Y, Z = -cartesian_direction  # Taking the negative as view_init specifies direction outward\n",
    "    R = torch.sqrt(X**2 + Y**2 + Z**2)\n",
    "    X, Y, Z = X/R, Y/R, Z/R  # normalization\n",
    "    azim = torch.rad2deg(torch.atan2(Y, X))\n",
    "    elev = torch.rad2deg(torch.arcsin(Z))\n",
    "    # Multiple angles to understand better\n",
    "    for ax, (mod_elev, mod_azim) in zip(axes, [[0,0],[-10, 60]]):\n",
    "        ax.view_init(elev + mod_elev, azim + mod_azim, 0)\n",
    "        ax.plot(points[:, :, 0], points[:, :, 1], points[:, :, 2], linewidth=0.2, markersize=2, marker='o')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "points, directions, depths = sample_ray_uniformally(rays[:10000:71, 0], rays[:10000:71, 1], 2.0, 10.0, 7, perturb=True)\n",
    "\n",
    "print(f\"{points.shape=} | {directions.shape=} | {depths.shape=}\")\n",
    "plot_ray_sampling(points, rays[0, 0], directions[:, 0, :].mean(0), \"Example of perturbed uniform samples along ray\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def sample_pdf(bins, weights, num_samples, deterministic=False):\n",
    "    weights = weights + 1e-5  # avoid nans later\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)  # Normalize PDF\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)  # Prepend 0 to have cdf->[0,1]\n",
    "\n",
    "    if deterministic:\n",
    "        u = torch.linspace(0.0, 1.0, steps=num_samples)\n",
    "        u = u.expand(list(cdf.shape[:-1]) + [num_samples])\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [num_samples])\n",
    "\n",
    "    # Inverting the CDF\n",
    "    u = u.contiguous()  # Need contigous memory layout for further operations\n",
    "    indexes = torch.searchsorted(cdf, u, right=True)  # Finding bins\n",
    "    # Need to ensure below and above don't leave bounds of bins\n",
    "    below = torch.max(torch.zeros_like(indexes-1), indexes-1)\n",
    "    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(indexes), indexes)\n",
    "    indexes = torch.stack([below, above], dim=-1)\n",
    "\n",
    "    # Gathering sampled bins and bound probabilites\n",
    "    shape = [indexes.shape[0], indexes.shape[1], cdf.shape[-1]]\n",
    "    cdf = torch.gather(cdf.unsqueeze(1).expand(shape), dim=2, index=indexes)\n",
    "    bins = torch.gather(bins.unsqueeze(1).expand(shape), dim=2, index=indexes)\n",
    "\n",
    "    # denominator is the size of the bins\n",
    "    denominator = cdf[..., 1] - cdf[..., 0]\n",
    "    denominator = torch.where(denominator < 1e-5, torch.ones_like(denominator), denominator)\n",
    "    denominator[denominator < 1e-5] = 1.0\n",
    "    # t gives the relative position inside the bins \n",
    "    t = (u - cdf[..., 0]) / denominator\n",
    "\n",
    "    samples = bins[..., 0] + t * (bins[..., 1] - bins[..., 0])\n",
    "    return samples\n",
    "\n",
    "\n",
    "bins = torch.linspace(0, 1, steps=11) # 10 intervals (bins)\n",
    "weights = torch.tensor([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.3, 0.25, 0.2, 0.1]) # Arbitrary weights\n",
    "weights = weights / torch.sum(weights, -1)\n",
    "\n",
    "# Expand the inputs to include a batch dimension\n",
    "bins = bins.unsqueeze(0) # Shape: [1, 11]\n",
    "weights = weights.unsqueeze(0) # Shape: [1, 10]\n",
    "samples = sample_pdf(bins, weights, 5)\n",
    "\n",
    "print(f\"{samples=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: STANDALONE\n",
    "def sample_ray_hierarchically(origins, directions, num_samples, bins, weights, deterministic=False):\n",
    "    depths = sample_pdf(bins, weights, num_samples, deterministic=deterministic)\n",
    "\n",
    "    points = origins[..., None, :] + directions[..., None, :] * depths[..., :, None]\n",
    "    # Expand directions to make NeRF input\n",
    "    directions = directions[..., None, :].expand(points.shape)\n",
    "    return points, directions, depths\n",
    "\n",
    "bins = torch.linspace(0, 1, 65).expand((10000 // 41 + 1, 65))  # Computed after uniform sampling\n",
    "weights = torch.randn((10000 // 41 + 1, 64))  # Retrieved from NeRF's sigma values\n",
    "points, directions, depths = sample_ray_hierarchically(rays[:10000:41, 0], rays[:10000:41, 1], 32, bins, weights)\n",
    "\n",
    "print(f\"{points.shape=} | {directions.shape=} | {depths.shape=}\")\n",
    "plot_ray_sampling(points, rays[0, 0], directions[:, 0, :].mean(0), \"Example of hierarchical samples along ray\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: NeRF() STANDALONE, compute_along_rays LIGHTNING MODULE\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, num_layers=8, hidden_size=256, in_coordinates=3, in_directions=3,\n",
    "                 skips=[4], coord_encode_freq=10, dir_encode_freq=4):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.in_coordinates = in_coordinates\n",
    "        self.in_directions = in_directions\n",
    "        self.skips = tuple(skips)\n",
    "\n",
    "        self.coordinate_encoder = PositionalEncoding(coord_encode_freq)\n",
    "        self.direction_encoder = PositionalEncoding(dir_encode_freq)\n",
    "\n",
    "        coord_dim = self.coordinate_encoder.get_out_dim(self.in_coordinates)\n",
    "        self.feature_mlp = nn.ModuleList([nn.Linear(coord_dim, hidden_size)])\n",
    "        # go until num_layers -1 as we already have the initial layer\n",
    "        for i in range(num_layers - 1):\n",
    "            self.feature_mlp.append(nn.Sequential(\n",
    "                # skip with +1 as we already have the initial layer\n",
    "                nn.Linear(hidden_size + (coord_dim if i+1 in self.skips else 0), hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ))\n",
    "\n",
    "        self.sigma_fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.color_preproc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        dir_dim = self.direction_encoder.get_out_dim(self.in_directions)\n",
    "        self.rgb_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size + dir_dim, hidden_size // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size // 2, 3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def get_device(self):\n",
    "        # Assumes all parameters are on the same device\n",
    "        return next(model.parameters()).device\n",
    "\n",
    "    def forward(self, coordinates, directions, skip_colors=False):\n",
    "        coordinates = self.coordinate_encoder(coordinates)\n",
    "        features = coordinates\n",
    "        for i, fc in enumerate(self.feature_mlp):\n",
    "            if i in self.skips:\n",
    "                features = torch.cat([features, coordinates], -1)\n",
    "            features = fc(features)\n",
    "\n",
    "        sigma = self.sigma_fc(features)\n",
    "\n",
    "        if skip_colors:\n",
    "            return sigma\n",
    "\n",
    "        directions = self.direction_encoder(directions)\n",
    "        features = self.color_preproc(features)\n",
    "        features = torch.cat([features, directions], dim=-1)\n",
    "        rgb = self.rgb_mlp(features)\n",
    "\n",
    "        return torch.cat([rgb, sigma], dim=-1)\n",
    "    \n",
    "    def compute_along_rays(self, origins, directions, near, far, coarse_samples, fine_samples, deterministic=True):\n",
    "        device = self.get_device()\n",
    "        # This function deviates from the original NeRF paper as the coarse and fine samples are processed by the same model\n",
    "        points, expanded_directions, coarse_depths = sample_ray_uniformally(origins, directions, near, far, coarse_samples)\n",
    "        coarse_out = self(points.to(device), expanded_directions.to(device)).to(torch.device('cpu'))\n",
    "        \n",
    "        # Bin bounds are halfway between sampled coordinates + near + far plane\n",
    "        bins = torch.cat([\n",
    "            torch.tensor(near, dtype=torch.float32).expand(origins.shape[0], 1),\n",
    "            (coarse_depths[..., 1:] + coarse_depths[..., :-1]) / 2,\n",
    "            torch.tensor(far, dtype=torch.float32).expand(origins.shape[0], 1),\n",
    "        ], -1)\n",
    "\n",
    "        points, expanded_directions, fine_depths = sample_ray_hierarchically(origins, directions, fine_samples, bins,\n",
    "                                                                             coarse_out[..., -1], deterministic=deterministic)\n",
    "        fine_out = self(points.to(device), expanded_directions.to(device)).to(torch.device('cpu'))\n",
    "\n",
    "        # deterministic ensures depth sorted output, if non-deterministic, sort manually as sortedness is required for volume rendering\n",
    "        if not deterministic:\n",
    "            fine_depths, idxs = torch.sort(fine_depths, dim=-1)\n",
    "            fine_out = fine_out[torch.arange(idxs.shape[0]).unsqueeze(1), idxs]\n",
    "\n",
    "        return coarse_out, coarse_depths, fine_out, fine_depths\n",
    "    \n",
    "\n",
    "model = NeRF().to(COMPUTE_DEVICE)\n",
    "print(f\"Output shape for origins + directions: {model(rays[:50, 0].to(COMPUTE_DEVICE), rays[:50, 1].to(COMPUTE_DEVICE)).shape}\")\n",
    "\n",
    "coarse_rgbs, coarse_depths, fine_rgbs, fine_depths = model.compute_along_rays(rays[:500, 0], rays[:500, 1], 0, 10.0, 64, 5, False)\n",
    "print(f\"Coarse output shape for hierarchical sampling: {coarse_rgbs.shape=} | {coarse_depths.shape=}\")\n",
    "print(f\"Fine output shape for hierarchical sampling:   {fine_rgbs.shape=} | {fine_depths.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: LIGHTNING MODULE\n",
    "def render_rays(rgbs, depths):\n",
    "    distances = depths[..., 1:] - depths[..., :-1]\n",
    "    # 1e10 ensures the last color is rendered no matter what\n",
    "    distances = torch.cat([distances, torch.Tensor([1e10]).expand(distances[...,:1].shape)], -1)\n",
    "    # directions already normalized at ray calculation, so distances correspond to world already\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-F.relu(rgbs[..., 3]) * distances)\n",
    "    # 1e10 ensures the last color is rendered no matter what\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1. - alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb = torch.sum(weights[..., None] * rgbs[..., :3], dim=-2)\n",
    "    depth = torch.sum(weights * depths, dim=-1)\n",
    "\n",
    "    return rgb, depth\n",
    "\n",
    "rendered_rgb, rendered_depth = render_rays(coarse_rgbs, coarse_depths)\n",
    "print(f\"Color and depth shapes after render: {rendered_rgb.shape=} | {rendered_depth.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning conversion proposal: LIGHTNING MODULE\n",
    "@torch.no_grad\n",
    "def render_image(model: NeRF, height, width, c2w, focal, near, far, batch_size=512):\n",
    "    intrinsic = torch.tensor([\n",
    "        [focal.item(), 0, width // 2],\n",
    "        [0, focal.item(), height // 2],\n",
    "        [0, 0, 1],\n",
    "    ], dtype=torch.float32)\n",
    "    origins, directions = create_rays(height, width, intrinsic, c2w)\n",
    "\n",
    "    origins = origins.flatten(0, -2)\n",
    "    directions = directions.flatten(0, -2)\n",
    "    data = TensorDataset(origins, directions)\n",
    "    data = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    image = []\n",
    "    for o, d in data:\n",
    "        _, _, rgbs, depths = model.compute_along_rays(o, d, near, far, 64, 64)\n",
    "        rgb, _ = render_rays(rgbs, depths)\n",
    "        image.append(rgb)\n",
    "\n",
    "    image = torch.cat(image, 0).reshape(height, width, -1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "render = render_image(NeRF().to(COMPUTE_DEVICE), 50, 50, c2ws[-1], focal, 2.0, 10.0)\n",
    "plt.imshow(render.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
